<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA');.lst-kix_62gtr7dgmd8s-6>li:before{content:"\0025cf  "}.lst-kix_62gtr7dgmd8s-7>li:before{content:"\0025cb  "}.lst-kix_62gtr7dgmd8s-0>li:before{content:"\0025cf  "}.lst-kix_62gtr7dgmd8s-8>li:before{content:"\0025a0  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_62gtr7dgmd8s-4{list-style-type:none}.lst-kix_62gtr7dgmd8s-2>li:before{content:"\0025a0  "}.lst-kix_62gtr7dgmd8s-3>li:before{content:"\0025cf  "}ul.lst-kix_62gtr7dgmd8s-5{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-2{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-3{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-0{list-style-type:none}.lst-kix_62gtr7dgmd8s-1>li:before{content:"\0025cb  "}ul.lst-kix_62gtr7dgmd8s-1{list-style-type:none}.lst-kix_62gtr7dgmd8s-5>li:before{content:"\0025a0  "}ul.lst-kix_62gtr7dgmd8s-8{list-style-type:none}.lst-kix_62gtr7dgmd8s-4>li:before{content:"\0025cb  "}ul.lst-kix_62gtr7dgmd8s-6{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-7{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c15{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:285.8pt;border-top-color:#000000;border-bottom-style:solid}.c9{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:134.2pt;border-top-color:#000000;border-bottom-style:solid}.c3{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.2pt;border-top-color:#000000;border-bottom-style:solid}.c5{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c19{padding-top:18pt;padding-bottom:6pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c25{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c10{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c22{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c36{border-spacing:0;border-collapse:collapse;margin-right:auto}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c13{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c26{padding-top:20pt;padding-bottom:6pt;line-height:1.15;text-align:left}.c12{background-color:#ffffff;font-size:8pt;font-family:"Times New Roman";font-weight:400}.c35{background-color:#ffffff;font-weight:400;font-size:10.5pt;font-family:"Roboto"}.c32{padding-top:14pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c45{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:justify}.c33{text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c28{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c41{background-color:#ffffff;font-size:9pt;color:#333333}.c42{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c38{background-color:#ffffff;font-size:10.5pt}.c44{color:#000000;font-size:20pt}.c11{color:inherit;text-decoration:inherit}.c34{padding:0;margin:0}.c23{font-size:9pt;font-style:italic}.c39{vertical-align:baseline;font-style:normal}.c40{color:#3c4043;text-decoration:none}.c20{orphans:2;widows:2}.c24{color:#666666;font-size:12pt}.c27{color:#000000;font-size:11pt}.c18{margin-left:36pt;padding-left:0pt}.c29{background-color:#ffffff;color:#222222}.c43{color:#000000;font-size:10pt}.c8{page-break-after:avoid}.c14{font-weight:700}.c30{height:16pt}.c17{height:12pt}.c7{height:0pt}.c31{font-size:12pt}.c37{font-size:11pt}.c21{height:11pt}.c46{color:#3c4043}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c42 doc-content"><h1 class="c20 c8 c26" id="h.2yawkmmk6pbg"><span class="c25 c44">Interdisciplinary Perspectives on Technologies for Mingling</span></h1><h4 class="c32 c20 c8" id="h.tmptoux9f1tn"><span class="c24 c25">The Future of Conversations and Mingling </span></h4><p class="c2"><span class="c4">Time: 2 May 2023</span></p><p class="c2"><span>Location : &nbsp;</span><span class="c13"><a class="c11" href="https://www.google.com/url?q=https://vakwerkhuis.com/&amp;sa=D&amp;source=editors&amp;ust=1682952691054010&amp;usg=AOvVaw2F1F4fA0jYYqfAayFCxa2L">Vakwerkhuis, Delft </a></span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.00px; height: 303.00px;"><img alt="" src="images/image1.png" style="width: 538.00px; height: 303.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c19 c8 c30" id="h.bf7z376z34o6"><span class="c16"></span></h2><p class="c10"><span class="c5"><a class="c11" href="#h.uobv7junyb4f">Registration and Participation:</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.gg8m22e8uqaw">Introduction</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.l3c9d5n4e4o4">Motivation</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.og4jawi1xhf5">Bring your posters!</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.5rwur0w63m4d">Schedule</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.2gbt7c8ah9zb">Invited Talks</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.35tfy9xgnrgx">MINGLE Project Talks</a></span></p><p class="c10"><span class="c5"><a class="c11" href="#h.r0r4rnwmcv0z">Poster Presentations</a></span></p><h2 class="c19 c8" id="h.uobv7junyb4f"><span class="c16">Registration and Participation: </span></h2><p class="c2"><span>Please register </span><span class="c13"><a class="c11" href="https://www.google.com/url?q=https://docs.google.com/forms/d/e/1FAIpQLSdii11cRmcQ9Y-fJQTgnAI8FK64ngxShm0_I27oHdR8tEsXLQ/viewform&amp;sa=D&amp;source=editors&amp;ust=1682952691055900&amp;usg=AOvVaw258fY9X_cJe3WdbeXQqQra">here </a></span><span class="c14">before April 25</span><span class="c4">. The main idea is to use this as a networking and idea exchange opportunity so registration is free!</span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span class="c4">The main aim of this event is to bring together researchers and stakeholders who may not have become united by such a common theme before. So physical presence is strongly encouraged. However, if you would like to join remotely, here is the link:</span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span class="c35 c39 c40">Join Zoom Meeting</span></p><p class="c2"><span class="c13 c35"><a class="c11" href="https://www.google.com/url?q=https://tudelft.zoom.us/j/97141920397?pwd%3DamxlMmR1d244b0piT0c0MnpXRXAxZz09&amp;sa=D&amp;source=editors&amp;ust=1682952691056584&amp;usg=AOvVaw2rpl2brGK2JWi5qmd4X7qC">https://tudelft.zoom.us/j/97141920397?pwd=amxlMmR1d244b0piT0c0MnpXRXAxZz09</a></span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span class="c35 c39 c40">Meeting ID: 971 4192 0397</span></p><p class="c2"><span class="c35 c46">Passcode: 661783</span></p><h2 class="c19 c8" id="h.gg8m22e8uqaw"><span class="c16">Introduction</span></h2><p class="c2"><span class="c4">Attending social networking events has been correlated with career success. Yet little is known about how or why they function well or how we could make them more useful for us. This is despite the fact that we spend substantial time and money to attend them. One of the major bottlenecks has been related to difficulties in observing such behaviour systematically. This has made it hard to develop theories to fully understand what happens in these crowds. Without the possibility to analyse them, technologies cannot be built to help us to make the most out of these experiences which can sometimes be anxiety inducing for some. </span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span class="c4">During the global pandemic of recent years, these spontaneous moments of conversational interaction were lost and led to people questioning whether we should bring serendipity and spontaneity back in other forms. What makes a conversation good? What makes them interesting enough to form new bonds or foster existing connections? How does this play out in groups? </span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span class="c4">This symposium aims to crack open the mysteries of mingling behaviour from multiple different perspectives. Moreover, understanding how to build technologies for such settings would enable us to bridge the gap in understanding behaviours in similar and even more commonplace activities such as the role of spontaneous discussions by the coffee machine at work or in public spaces. </span></p><h2 class="c19 c8" id="h.l3c9d5n4e4o4"><span class="c16">Motivation</span></h2><p class="c2"><span class="c4">The day aims to present an overarching view of the research results of the NWO funded Vidi project MINGLE (Modelling Social Group Dynamics and Interaction Quality in Complex Scenes using Multi-Sensor Analysis of Non-Verbal Behaviour). Whilst this is the closing event of the MINGLE project, it is also aimed as a new beginning. The research results of MINGLE will feed into a new ERC Consolidator grant funded project NEON (Nonverbal Intention Modelling) which will focus on the analysis of intention, particularly &nbsp;in mingling settings. So we are looking for new perspectives to enrich the new research journey.</span></p><p class="c2 c21"><span class="c4"></span></p><p class="c2"><span class="c4">Since this is a first of its kind event, we want to kickstart a new kind of community that explores important research questions, solutions, and needs that can aid spontaneous social connection making. </span></p><h2 class="c19 c8" id="h.og4jawi1xhf5"><span class="c16">Bring your posters!</span></h2><p class="c2"><span>Do you work on a </span><span>related</span><span class="c4">&nbsp;topic? topics include but are not limited to:</span></p><ul class="c34 lst-kix_62gtr7dgmd8s-0 start"><li class="c2 c18 li-bullet-0"><span>Individual and Social </span><span>Behaviour</span><span class="c4">&nbsp;Understanding (e.g. gaze, gestures, postures, speech detection, paralinguistics, multimodal behaviour analysis)</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Cognitive states (e.g. affect, memory, trust)</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Cooperation and Collaboration in groups, organizations</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Interpersonal factors: engagement, rapport, synchrony, and mimicry</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Ethics and Privacy issues</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Interactive agents (e.g. social robots, virtual agents)</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Wearable/ubiquitous sensing</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Data collection techniques</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Data annotation techniques</span></li><li class="c2 c18 li-bullet-0"><span class="c4">Technologies to enable or enhance social interactions</span></li></ul><h2 class="c19 c8" id="h.5rwur0w63m4d"><span class="c16">Schedule </span></h2><p class="c2 c21"><span class="c4"></span></p><a id="t.10d238e14fbd1a73b3c1b40d346e8b4e01cffaca"></a><a id="t.0"></a><table class="c36"><thead><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">Time</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c21"><span class="c4"></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Speaker</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">9:15</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c6">Walk in: tea, coffee, and refreshments are available</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c28 c23 c14">Chair: Chirag Raman</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">9:45</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c4">Opening</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Hayley Hung </span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">10:00</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c2"><span class="c13"><a class="c11" href="#h.ni0z9p3anb89">Head and Body behaviour Estimation with F-formations</a></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Stephanie Tan</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">10:20</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span class="c13"><a class="c11" href="#h.xwayb5vb9b6j">Studies on Social Interaction using Wearables and Theatre</a></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Jamie A Ward</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">10:55</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c6">Coffee Break/ hang poster</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c28 c23 c14">Chair: Hayley Hung</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">11:10</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c4">Data Collection and Annotation of Complex Conversational Scenes</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Jose Vargas Quiros and &nbsp;Chirag Raman</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">11:30</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span class="c13"><a class="c11" href="#h.on2dthapwzsb">Towards Gaze Analysis in the Wild</a></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Jean-Marc Odobez</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">12:05</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c6">Buffet Lunch</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c23 c14 c28">Chair: Hayley Hung</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">13:05</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span class="c13"><a class="c11" href="#h.yi04rxhqk484">Robots within Groups of People</a></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Xavier Alameda Pineda</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">13:40</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c4">F-formation Modelling and Behavioural Cue Forecasting.</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Chirag Raman</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">14:00</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span class="c13"><a class="c11" href="#h.7q9szsni17p9">Socially significant bodily rhythms</a></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Wim Pouw</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">14:35</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c6">Coffee Break</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c28 c23 c14">Chair: Bernd Dudzik</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">14:50</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c4">Estimating Conversational Enjoyment and Learning Multiple Truths about Laughter</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Chirag Raman and Hayley Hung</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">15:10</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span class="c13"><a class="c11" href="#h.gkwxxgl0hets">Understanding Expertise Search Strategies at Networking Events: &nbsp;An Exploratory Study Using Sociometric Badges</a></span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Balint Dioszegi</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">15:45</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span>The ConfFlow Application: </span><span class="c38">Encouraging New Diverse Collaborations by Helping Researchers Find Each Other at a Conference</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c4">Hayley Hung</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">15:50</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0 c20"><span class="c6">Coffee Break</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c14 c23">Chair: Bernd Dudzik</span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">16:05</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c4">(Panel) Discussion: &nbsp;The main aim of this discussion is to reflect on the talks of the day and to discuss how to build technologies and carry out research to support spontaneous interactions</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0 c21"><span class="c4"></span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">17.10</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c6">Drinks reception and poster session</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0 c21"><span class="c4"></span></p></td></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">18:15</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span class="c4">Group walk to the dinner location</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0 c21"><span class="c4"></span></p></td><tbody></tbody></tr><tr class="c7"><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c4">18:30 </span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c0"><span>Dinner in town </span><span class="c13"><a class="c11" href="https://www.google.com/url?q=https://huszar.nl/en/&amp;sa=D&amp;source=editors&amp;ust=1682952691073424&amp;usg=AOvVaw0fl5vrjCYzJkCI5UgJ07s4">(Huszar</a></span><span class="c4">) located 5 minutes from Delft Central Station and a 15 minute walk from Vakwerkhuis.</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0 c21"><span class="c4"></span></p></td></tr></thead></table><h4 class="c20 c8 c32" id="h.svdn3qvno0ni"><span class="c25 c24">PhD Thesis Defense of Stephanie Tan on May 3</span></h4><p class="c2"><span class="c4">The PhD thesis defense of Stephanie Tan will start at 9.30 (layman&rsquo;s presentation) before the defense at 10am in the Aula on the campus of TUDelft.</span></p><h2 class="c8 c19" id="h.2gbt7c8ah9zb"><span class="c16">Invited Talks</span></h2><h4 class="c1 c8" id="h.xwayb5vb9b6j"><span class="c14">Talk Title:</span><span>&nbsp;Studies on Social Interaction using Wearables and Theatre<br></span><span class="c14">Speaker: </span><span>&nbsp;</span><span class="c13"><a class="c11" href="https://www.google.com/url?q=https://jamieward.net/&amp;sa=D&amp;source=editors&amp;ust=1682952691074671&amp;usg=AOvVaw0TMJfS-PpFrQokwqDvgzDq">Dr Jamie A Ward</a></span><span class="c25 c24">, Senior Lecturer in Computer Science at Goldsmiths University of London</span></h4><p class="c1"><span class="c14">Abstract: </span><span class="c4">Measuring detailed information on how people move, see, and think during realistic social situations can be a powerful method in studying social behaviour and cognition. However, &nbsp;measurement-driven research can be limited by the available technology, &nbsp;with bulky equipment and rigid constraints often confining such work to the laboratory, thus limiting the ecological validity of any findings. &nbsp;Together with colleagues at Goldsmiths, UCL, and Keio University, I have been working on several projects that use wearable sensing to take this research out of the laboratory and into the real world -- while on the way, stopping off at the theatre. In this talk, I will give a brief overview of some of our work, and try to show how the paradigm of &nbsp;&#39;theatre as a laboratory&#39;, might provide a way forward, both for &nbsp;research in social cognition, and in wearable sensing.</span></p><h4 class="c1 c8" id="h.yi04rxhqk484"><span class="c14">Talk Title:</span><span>&nbsp;Robots within Groups of People<br></span><span class="c24 c14">Speaker: </span><span class="c24">&nbsp;</span><span class="c13 c31"><a class="c11" href="https://www.google.com/url?q=http://xavirema.eu/&amp;sa=D&amp;source=editors&amp;ust=1682952691075391&amp;usg=AOvVaw3V8M0cHe4FykfdDPURginS">Xavier Alameda-Pineda</a></span><span class="c25 c24">, INRIA, France</span></h4><p class="c1"><span class="c14">Abstract: </span><span>One of the prominent applications of understanding social human behavior is the development of systems that can interpret, react to, and synthesize behavioral cues, and therefore take and be part of social interactions. In this very general context, social autonomous systems, e.g. social robotics, are a very challenging and complex research area that has received increasing attention over the past years. In this talk, I will be discussing the conception of machine learning methods allowing to perceive, generate and synthesize certain human behavioral cues. The tackled tasks will range from speech enhancement to meta-training for social navigation, and for each of them I will focus on one technical detail that is crucial for the conception of the machine learning model and associated training algorithm.</span></p><h4 class="c1 c8" id="h.gkwxxgl0hets"><span class="c14">Talk Title:</span><span>&nbsp;Understanding Expertise Search Strategies at Networking Events: &nbsp;An Exploratory Study Using Sociometric Badges<br></span><span class="c24 c14">Speaker: </span><span class="c13 c31"><a class="c11" href="https://www.google.com/url?q=https://www.linkedin.com/in/b%25C3%25A1lint-di%25C3%25B3szegi-8292734b/?originalSubdomain%3Duk&amp;sa=D&amp;source=editors&amp;ust=1682952691076191&amp;usg=AOvVaw3GudM2EWMKcvv5v-wMdm5j">Balint Dioszegi</a></span><span class="c24">, University of Greenwich, UK</span></h4><p class="c1"><span class="c14">Abstract: </span><span class="c4">In this study we ask how individuals search for experts at networking events. Building on the intuition that individuals&rsquo; propensities to engage in certain search actions, as well as their effectiveness in locating experts, will depend on the quality and salience of the metaknowledge they have about others, we conducted an expert search game as a field experiment in which we randomly assigned participants &ndash; researchers in a multinational corporation &ndash; to one of three treatment conditions, reflecting varying degrees of search planning. Based on data from sociometric badges, we derive a taxonomy of the micro-decisions individuals make at events. We find that letting others approach yields more referrals than taking the initiative in starting conversations, and that planning increases the tendency to maintain such initiative even when doing so is ineffective &ndash; a possible manifestation of the Einstellung effect.</span></p><h4 class="c1 c8" id="h.on2dthapwzsb"><span class="c14">Talk Title:</span><span>&nbsp;Towards gaze analysis in the wild<br></span><span class="c24 c14">Speaker: </span><span class="c13 c31"><a class="c11" href="https://www.google.com/url?q=https://www.idiap.ch/~odobez/&amp;sa=D&amp;source=editors&amp;ust=1682952691077041&amp;usg=AOvVaw0QbGmfcdBSh0ZDDsDGGdxX">Jean-Marc Odobez</a></span><span class="c25 c24">, Idiap Research Institute and EPFL, Switzerland</span></h4><p class="c20 c45"><span class="c14">Abstract: </span><span class="c4">As a display of attention and interest, gaze is a fundamental cue in understanding people&#39;s activities, behaviors, and state of mind, and plays an important role in many applications and research fields, for the design of intuitive human computer or robot interfaces, or for medical diagnosis like for assessing Autism Spectrum Disorders (ASD) in children. Gaze (estimating the 3D line of sight) and Visual Focus of Attention (VFOA) estimation, however, are challenging, even for humans. It often requires not only analysing the person&#39;s face and eyes, but also the scene content including the 3D scene structure and the person&rsquo;s situation to detect obstructions in the line of sight or apply attention priors that humans typically have when observing others. In this presentation, I will present methods that address these challenges: first, a method that leverages standard activity-related priors about gaze to perform online calibration; secondly, an approach for VFOA inference which casts the scene in the 3D field of view of a person, enabling the use of audio-visual information as well as dealing with an arbitrary number of targets; and third, moving towards gaze estimation in the wild, an approach for the gaze-following task explicitly leveraging derived multimodal cues like depth and pose.</span></p><p class="c1 c21"><span class="c4"></span></p><h4 class="c1 c8" id="h.7q9szsni17p9"><span class="c14">Talk Title:</span><span>&nbsp;Socially significant bodily rhythms<br></span><span class="c14">Speaker: </span><span class="c13"><a class="c11" href="https://www.google.com/url?q=https://wimpouw.com/&amp;sa=D&amp;source=editors&amp;ust=1682952691077753&amp;usg=AOvVaw0YcDUJz_Cg4lh7w37d9IUl">Wim Pouw</a></span><span class="c25 c24">, Radboud University, Netherlands</span></h4><p class="c1"><span class="c14">Abstract: </span><span class="c4">In this talk I will highlight that bodily constraints can be recruited to do communicative work. I will highlight that the process by which a body is put to work often entails deviations from endogenous rhythms that emerge in interaction with the (non-social) environment. Throughout the talk I will entertain the idea that affective communication in some sense depends on significant deviations from one&#39;s bodily stabilities. I will overview a research program called gesture-speech physics that aligns with this idea. This research is about how the pulse-quality of upper limb gestures produce forces through acceleration, thereby physically and functionally perturbing speech processes; grounding gesture&rsquo;s phylogeny, ontogeny, and cognition, in physiology. I also discuss recent research with Siamang apes (Symphalangus syndactylus) which will support the argument that there is cross-species continuity in how bodies are put to work in vocal communication. I conclude therefore that expressive bodies are, it turns out, just moving about, but in more significant ways than previously thought.</span></p><p class="c1 c21"><span class="c4"></span></p><h2 class="c1 c8" id="h.35tfy9xgnrgx"><span class="c16">MINGLE Project Talks</span></h2><h4 class="c1 c8" id="h.ni0z9p3anb89"><span class="c14">Talk Title: </span><span class="c27">Head and Body behaviour Estimation with F-formations</span><span><br></span><span class="c14">Speaker: </span><span class="c25 c24">&nbsp;Stephanie Tan</span></h4><p class="c1"><span class="c6">Abstract: </span></p><p class="c1"><span class="c29">In recent years, new domains such as social signal processing and social computing have emerged at the intersection of computer science, human behavioral modeling, and robotics. The aim of these fields is to achieve machine perception of social intelligence, such as understanding behavioral cues of humans (e.g., body language) and complex social relations and attitudes (e.g., dominance, rapport). Challenges towards building such systems include data acquisition with appropriate sensing capabilities for capturing in-the-wild human data, as well as modelling approaches that account for data from multiple modalities (vision, audio, motion) and address context-awareness. In light of these challenges, I will present my work on (1) head and body orientation estimation using sparse weak labels from wearable sensors, (2) joint head orientation estimation in conversation groups, and (3) conversation group detection in social interaction scenes, in addition to an overview on the related data-oriented contributions. I motivate these three tasks from the perspective of individual-level, group-level, and scene-level behavior understanding, and conclude with some open questions related to automated analysis of social behaviors. &nbsp;</span></p><p class="c1"><span class="c6">Associated Paper(s):</span></p><p class="c1"><span class="c22 c12">S. Tan, D. M. J. Tax, H. Hung, Conversation group detection with spatio-temporal context,Proceedings of 2022 International Conference on Multimedia (ICMI) (2022), Pages 170&ndash;180, Oral Presentation</span></p><p class="c1"><span class="c22 c12">S. Tan, D. M. J. Tax, H. Hung, Multimodal joint head orientation estimation in interacting groups via proxemics and interaction dynamics, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) (2021), Vol.5, No.1, 1-22.</span></p><p class="c1"><span class="c12 c22">S. Tan, D. M. J. Tax, H. Hung, Head and body orientation estimation with sparse weak labels in free standing conversational settings, Understanding Social Behavior in Dyadic and Small Group Interactions, Proceedings of Machine Learning Research (2021), 179-203. Presented at ICCV 2021 Understanding Social Behavior in Dyadic and Small Group Interactions Workshop</span></p><p class="c1 c21"><span class="c4"></span></p><p class="c1 c21"><span class="c4"></span></p><p class="c1 c21"><span class="c4"></span></p><h4 class="c1 c8" id="h.e5sevkx583y9"><span class="c14">Talk Title:Data Collection and Annotation of Complex Conversational Scenes</span><span><br></span><span class="c14">Speaker: </span><span class="c25 c24">&nbsp;Jose Vargas Quiros and Chirag Raman</span></h4><p class="c1"><span class="c6">Abstract: TBA</span></p><p class="c1"><span class="c6">Associated Paper(s):</span></p><p class="c1"><span class="c12">Quiros, J. V., Tan, S., Raman, C., Cabrera-Quiros, L., &amp; Hung, H. (2022, March). Covfee: an extensible web framework for continuous-time annotation of human behavior. In </span><span class="c12">Understanding Social Behavior in Dyadic and Small Group Interactions</span><span class="c22 c12">&nbsp;(pp. 265-293). PMLR.</span></p><p class="c1"><span class="c12">Raman, C., Tan, S., &amp; Hung, H. (2020, October). A modular approach for synchronized wireless multimodal multisensor data acquisition in highly dynamic social settings. In </span><span class="c12">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c22 c12">&nbsp;(pp. 3586-3594).</span></p><p class="c1"><span class="c12">Raman, C., Vargas Quiros, J., Tan, S., Islam, A., Gedik, E., &amp; Hung, H. (2022). ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine Analysis of Free-Standing Social Interactions in the Wild. </span><span class="c12">Advances in Neural Information Processing Systems</span><span class="c12">, </span><span class="c12">35</span><span class="c22 c12">, 23701-23715.</span></p><p class="c1"><span class="c12">Raman, C., Tan, S., &amp; Hung, H. (2020, October). A modular approach for synchronized wireless multimodal multisensor data acquisition in highly dynamic social settings. In </span><span class="c12">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c22 c12">&nbsp;(pp. 3586-3594).</span></p><h4 class="c1 c8 c17" id="h.ehxysfraymph"><span class="c25 c24"></span></h4><h4 class="c1 c8" id="h.ibjqkwi20om9"><span class="c14">Talk Title: F-formation Modelling and Behavioural Cue Forecasting.</span><span><br></span><span class="c14">Speaker: </span><span class="c25 c24">&nbsp;Chirag Raman</span></h4><p class="c1"><span class="c6">Abstract:</span></p><p class="c1"><span class="c6">Associated Paper(s):</span></p><p class="c1"><span class="c12">Raman, C., Hung, H., &amp; Loog, M. (2023, February). Social processes: Self-supervised meta-learning over conversational groups for forecasting nonverbal social cues. In Computer Vision&ndash;ECCV 2022 Workshops: Tel Aviv, Israel, October 23&ndash;27, 2022, Proceedings, Part III (pp. 639-659). Cham: Springer Nature Switzerland.</span></p><h4 class="c1 c8 c17" id="h.68svupq4mdnd"><span class="c33 c24 c14"></span></h4><h4 class="c1 c8" id="h.pj43nnn7mbbb"><span class="c14">Talk Title: Estimating Conversational Enjoyment and Learning Multiple Truths about Laughter</span><span><br></span><span class="c14">Speaker: </span><span class="c25 c24">&nbsp;Chirag Raman and Hayley Hung</span></h4><p class="c1"><span class="c6">Abstract: TBA</span></p><p class="c1"><span class="c6">Associated Paper(s):</span></p><p class="c1"><span class="c12">Raman, C., Prabhu, N. R., &amp; Hung, H. (2023). Perceived Conversation Quality in Spontaneous Interactions. </span><span class="c22 c12">IEEE Transactions on Affective Computing</span></p><p class="c1"><span class="c22 c12">Vargas-Quiros, J., Cabrera-Quiros, L., Oertel, C., &amp; Hung, H. (2022). Impact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild. arXiv preprint arXiv:2211.00794. To appear, IEEE Transactions on Affective Computing</span></p><p class="c1"><span class="c12">Quiros, J. D. V., Kapcak, O., Hung, H., &amp; Cabrera-Quiros, L. (2021). Individual and joint body movement assessed by wearable sensing as a predictor of attraction in speed dates. IEEE Transactions on Affective Computing.</span></p><h4 class="c1 c8 c17" id="h.6ltyb6lvuyw2"><span class="c33 c24 c14"></span></h4><h4 class="c1 c8 c17" id="h.jn4ivz8tdx76"><span class="c33 c24 c14"></span></h4><h4 class="c1 c8 c17" id="h.5y5yk8iwr3u9"><span class="c24 c14 c33"></span></h4><h4 class="c1 c8" id="h.xq5n2el5kwma"><span class="c14">Talk Title: The ConfFlow Application: Encouraging New Diverse Collaborations by Helping Researchers Find Each Other at a Conference</span><span><br></span><span class="c14">Speaker: </span><span class="c25 c24">&nbsp;Hayley Hung</span></h4><p class="c1"><span class="c14">Abstract: </span><span class="c41">We often find other collaborators by chance at a conference or by looking for them specifically through their papers. However, sometimes hidden potential social connections might exist between different researchers that cannot be immediately observed because the keywords we use might not always represent the entire space of similar research interests.ConfFlow is an online application that offers an alternative perspective on finding new research connections. It is designed to help researchers find others at conferences with complementary research interests for collaboration. With ConfFlow we take a data-driven approach by using something similar to the Toronto Paper Matching System (TPMS), used to identify suitable reviewers for papers, to construct a similarity embedding space for researchers to find other researchers. </span></p><p class="c1"><span class="c14 c37">Associated Paper(s):</span><span><br></span><span class="c12">H. Hung and E. Gedik, &ldquo;Encouraging Scientific Collaborations with ConfFlow 2021&rdquo;, SIGMM Records, </span><span class="c13 c12"><a class="c11" href="https://www.google.com/url?q=https://records.sigmm.org/2022/04/20/encouraging-scientific-collaborations-with-confflow-2021/&amp;sa=D&amp;source=editors&amp;ust=1682952691082345&amp;usg=AOvVaw0Ot4g9F7Hdp682NE2sA8RC">https://records.sigmm.org/2022/04/20/encouraging-scientific-collaborations-with-confflow-2021/</a></span><span class="c22 c12">, 2022</span></p><p class="c1"><span class="c12">H.Hung and E.Gedik, &ldquo;Encouraging more Diverse Scientific Collaborations with the ConfFlow application&rdquo;, SIGMM Records, </span><span class="c12"><a class="c11" href="https://www.google.com/url?q=https://records.sigmm.org/2021/06/10/encouraging-more-diverse-scientific-collaborations-with-the-confflow-application/&amp;sa=D&amp;source=editors&amp;ust=1682952691082721&amp;usg=AOvVaw0ee7Zsf6mU8QvRv7BD5jE2">https://records.sigmm.org/2021/06/10/encouraging-more-diverse-scientific-collaborations-with-the-confflow-application/</a></span><span class="c22 c12">, 2021</span></p><p class="c2"><span class="c12">Gedik, E., &amp; Hung, H. (2020, October). ConfFlow: A Tool to Encourage New Diverse Collaborations. In </span><span class="c12">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c12">&nbsp;(pp. 4562-4564).</span></p><p class="c1 c21"><span class="c22 c12"></span></p><h2 class="c1 c8" id="h.r0r4rnwmcv0z"><span class="c16"><br>Poster Presentations</span></h2><h4 class="c1 c8" id="h.rb9ojqsjmy31"><span class="c25 c24">Presenter name and affiliation:<br>Poster Title and Abstract:TBA</span></h4><h2 class="c1 c8 c30" id="h.t8ene8slce3y"><span class="c22 c12"></span></h2><p class="c2 c21"><span class="c4"></span></p><h2 class="c1 c8 c30" id="h.5htxyzvbch1o"><span class="c16"></span></h2><h4 class="c1 c8 c17" id="h.2ou0r5v8ze2o"><span class="c25 c24"></span></h4></body></html>
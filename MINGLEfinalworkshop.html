<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA');.lst-kix_62gtr7dgmd8s-6>li:before{content:"\0025cf  "}.lst-kix_62gtr7dgmd8s-7>li:before{content:"\0025cb  "}.lst-kix_62gtr7dgmd8s-0>li:before{content:"\0025cf  "}.lst-kix_62gtr7dgmd8s-8>li:before{content:"\0025a0  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_62gtr7dgmd8s-4{list-style-type:none}.lst-kix_62gtr7dgmd8s-2>li:before{content:"\0025a0  "}.lst-kix_62gtr7dgmd8s-3>li:before{content:"\0025cf  "}ul.lst-kix_62gtr7dgmd8s-5{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-2{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-3{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-0{list-style-type:none}.lst-kix_62gtr7dgmd8s-1>li:before{content:"\0025cb  "}ul.lst-kix_62gtr7dgmd8s-1{list-style-type:none}.lst-kix_62gtr7dgmd8s-5>li:before{content:"\0025a0  "}ul.lst-kix_62gtr7dgmd8s-8{list-style-type:none}.lst-kix_62gtr7dgmd8s-4>li:before{content:"\0025cb  "}ul.lst-kix_62gtr7dgmd8s-6{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-7{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c5{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:134.2pt;border-top-color:#000000;border-bottom-style:solid}.c7{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.2pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:285.8pt;border-top-color:#000000;border-bottom-style:solid}.c24{background-color:#ffffff;color:#3c4043;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Roboto";font-style:normal}.c11{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c2{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c0{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c35{background-color:#ffffff;font-weight:400;vertical-align:baseline;font-size:10.5pt;font-family:"Roboto";font-style:normal}.c36{color:#000000;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:italic}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c25{color:#666666;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c22{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c1{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c10{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c26{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c34{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c23{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c4{background-color:#ffffff;font-size:8pt;font-family:"Times New Roman";font-weight:400}.c41{border-spacing:0;border-collapse:collapse;margin-right:auto}.c16{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c46{padding-top:20pt;padding-bottom:6pt;line-height:1.15;text-align:left}.c38{padding-top:18pt;padding-bottom:6pt;line-height:1.15;text-align:left}.c32{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c28{font-weight:400;font-size:10pt;font-family:"Arial"}.c20{background-color:#ffffff;font-size:9pt;color:#333333}.c30{font-weight:400;font-size:20pt;font-family:"Arial"}.c27{color:#000000;font-size:11pt}.c29{orphans:2;widows:2}.c44{font-weight:400;font-family:"Roboto"}.c39{background-color:#ffffff;font-size:10.5pt}.c42{background-color:#ffffff;color:#222222}.c3{color:inherit;text-decoration:inherit}.c31{padding:0;margin:0}.c40{font-size:9pt;font-style:italic}.c17{height:12pt}.c15{page-break-after:avoid}.c45{color:#3c4043}.c21{height:11pt}.c33{font-size:12pt}.c18{height:16pt}.c43{font-size:11pt}.c8{font-weight:700}.c12{height:0pt}.c37{color:#666666}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c32 doc-content"><h1 class="c29 c15 c46" id="h.2yawkmmk6pbg"><span class="c23 c30">Interdisciplinary Perspectives on Technologies for Mingling</span></h1><h4 class="c0" id="h.tmptoux9f1tn"><span class="c2">The Future of Conversations and Mingling </span></h4><p class="c14"><span class="c6">Time: 2 May 2023</span></p><p class="c14"><span>Location : &nbsp;</span><span class="c16"><a class="c3" href="https://www.google.com/url?q=https://vakwerkhuis.com/&amp;sa=D&amp;source=editors&amp;ust=1682427983740542&amp;usg=AOvVaw0yiKh9BEN7CtXChK--NHVp">Vakwerkhuis, Delft </a></span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.00px; height: 303.00px;"><img alt="" src="images/image1.png" style="width: 538.00px; height: 303.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c38 c29 c15 c18" id="h.bf7z376z34o6"><span class="c13"></span></h2><p class="c34"><span class="c26 c16"><a class="c3" href="#h.uobv7junyb4f">Registration and Participation:</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.gg8m22e8uqaw">Introduction</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.l3c9d5n4e4o4">Motivation</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.og4jawi1xhf5">Bring your posters!</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.5rwur0w63m4d">Schedule</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.2gbt7c8ah9zb">Invited Talks</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.35tfy9xgnrgx">MINGLE Project Talks</a></span></p><p class="c34"><span class="c26 c16"><a class="c3" href="#h.r0r4rnwmcv0z">Poster Presentations</a></span></p><h2 class="c29 c15 c38" id="h.uobv7junyb4f"><span class="c13">Registration and Participation: </span></h2><p class="c14"><span>Please register </span><span class="c16"><a class="c3" href="https://www.google.com/url?q=https://docs.google.com/forms/d/e/1FAIpQLSdii11cRmcQ9Y-fJQTgnAI8FK64ngxShm0_I27oHdR8tEsXLQ/viewform&amp;sa=D&amp;source=editors&amp;ust=1682427983742177&amp;usg=AOvVaw3R55u9Irmb81377s6qhJDR">here </a></span><span class="c8">before April 25</span><span class="c6">. The main idea is to use this as a networking and idea exchange opportunity so registration is free!</span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span class="c6">The main aim of this event is to bring together researchers and stakeholders who may not have become united by such a common theme before. So physical presence is strongly encouraged. However, if you would like to join remotely, here is the link:</span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span class="c24">Join Zoom Meeting</span></p><p class="c14"><span class="c16 c39 c44"><a class="c3" href="https://www.google.com/url?q=https://tudelft.zoom.us/j/97141920397?pwd%3DamxlMmR1d244b0piT0c0MnpXRXAxZz09&amp;sa=D&amp;source=editors&amp;ust=1682427983742649&amp;usg=AOvVaw2EV54uHlDQo02xJjunlzKs">https://tudelft.zoom.us/j/97141920397?pwd=amxlMmR1d244b0piT0c0MnpXRXAxZz09</a></span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span class="c24">Meeting ID: 971 4192 0397</span></p><p class="c14"><span class="c44 c39 c45">Passcode: 661783</span></p><h2 class="c38 c29 c15" id="h.gg8m22e8uqaw"><span class="c13">Introduction</span></h2><p class="c14"><span class="c6">Attending social networking events has been correlated with career success. Yet little is known about how or why they function well or how we could make them more useful for us. This is despite the fact that we spend substantial time and money to attend them. One of the major bottlenecks has been related to difficulties in observing such behaviour systematically. This has made it hard to develop theories to fully understand what happens in these crowds. Without the possibility to analyse them, technologies cannot be built to help us to make the most out of these experiences which can sometimes be anxiety inducing for some. </span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span class="c6">During the global pandemic of recent years, these spontaneous moments of conversational interaction were lost and led to people questioning whether we should bring serendipity and spontaneity back in other forms. What makes a conversation good? What makes them interesting enough to form new bonds or foster existing connections? How does this play out in groups? </span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span class="c6">This symposium aims to crack open the mysteries of mingling behaviour from multiple different perspectives. Moreover, understanding how to build technologies for such settings would enable us to bridge the gap in understanding behaviours in similar and even more commonplace activities such as the role of spontaneous discussions by the coffee machine at work or in public spaces. </span></p><h2 class="c38 c29 c15" id="h.l3c9d5n4e4o4"><span class="c13">Motivation</span></h2><p class="c14"><span class="c6">The day aims to present an overarching view of the research results of the NWO funded Vidi project MINGLE (Modelling Social Group Dynamics and Interaction Quality in Complex Scenes using Multi-Sensor Analysis of Non-Verbal Behaviour). Whilst this is the closing event of the MINGLE project, it is also aimed as a new beginning. The research results of MINGLE will feed into a new ERC Consolidator grant funded project NEON (Nonverbal Intention Modelling) which will focus on the analysis of intention, particularly &nbsp;in mingling settings. So we are looking for new perspectives to enrich the new research journey.</span></p><p class="c14 c21"><span class="c6"></span></p><p class="c14"><span class="c6">Since this is a first of its kind event, we want to kickstart a new kind of community that explores important research questions, solutions, and needs that can aid spontaneous social connection making. </span></p><h2 class="c38 c29 c15" id="h.og4jawi1xhf5"><span class="c13">Bring your posters!</span></h2><p class="c14"><span>Do you work on a </span><span>related</span><span class="c6">&nbsp;topic? topics include but are not limited to:</span></p><ul class="c31 lst-kix_62gtr7dgmd8s-0 start"><li class="c11 li-bullet-0"><span>Individual and Social </span><span>Behaviour</span><span class="c6">&nbsp;Understanding (e.g. gaze, gestures, postures, speech detection, paralinguistics, multimodal behaviour analysis)</span></li><li class="c11 li-bullet-0"><span class="c6">Cognitive states (e.g. affect, memory, trust)</span></li><li class="c11 li-bullet-0"><span class="c6">Cooperation and Collaboration in groups, organizations</span></li><li class="c11 li-bullet-0"><span class="c6">Interpersonal factors: engagement, rapport, synchrony, and mimicry</span></li><li class="c11 li-bullet-0"><span class="c6">Ethics and Privacy issues</span></li><li class="c11 li-bullet-0"><span class="c6">Interactive agents (e.g. social robots, virtual agents)</span></li><li class="c11 li-bullet-0"><span class="c6">Wearable/ubiquitous sensing</span></li><li class="c11 li-bullet-0"><span class="c6">Data collection techniques</span></li><li class="c11 li-bullet-0"><span class="c6">Data annotation techniques</span></li><li class="c11 li-bullet-0"><span class="c6">Technologies to enable or enhance social interactions</span></li></ul><h2 class="c38 c29 c15" id="h.5rwur0w63m4d"><span class="c13">Schedule </span></h2><p class="c14"><span class="c6">Please note the schedule is not yet finalised and may be subject to slight shifts!</span></p><a id="t.10d238e14fbd1a73b3c1b40d346e8b4e01cffaca"></a><a id="t.0"></a><table class="c41"><thead><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">Time</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c21"><span class="c6"></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Speaker</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">9:15</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">Walk in: tea, coffee, and refreshments are available</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c36 c8">Chair: Chirag Raman</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">9:45</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">Opening</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Hayley Hung </span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">10:00</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c14"><span class="c16"><a class="c3" href="#h.ni0z9p3anb89">Head and Body behaviour Estimation with F-formations</a></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Stephanie Tan</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">10:20</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span class="c16"><a class="c3" href="#h.xwayb5vb9b6j">Studies on Social Interaction using Wearables and Theatre</a></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Jamie A Ward</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">10:55</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">Coffee Break/ hang poster</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c36 c8">Chair: Hayley Hung</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">11:10</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">Data Collection and Annotation of Complex Conversational Scenes</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Jose Vargas Quiros and &nbsp;Chirag Raman</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">11:30</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span class="c16"><a class="c3" href="#h.on2dthapwzsb">Towards Gaze Analysis in the Wild</a></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Jean-Marc Odobez</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">12:05</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">Buffet Lunch</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c36 c8">Chair: Hayley Hung</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">13:05</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span class="c16"><a class="c3" href="#h.yi04rxhqk484">Robots within Groups of People</a></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Xavier Alameda Pineda</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">13:40</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">F-formation Modelling and Behavioural Cue Forecasting.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Chirag Raman</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">14:00</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span class="c16"><a class="c3" href="#h.7q9szsni17p9">Socially significant bodily rhythms</a></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Wim Pouw</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">14:35</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">Coffee Break</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c8 c36">Chair: Bernd Dudzik</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">14:50</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">Estimating Conversational Enjoyment and Learning Multiple Truths about Laughter</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Chirag Raman and Hayley Hung</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">15:10</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span class="c16"><a class="c3" href="#h.gkwxxgl0hets">Understanding Expertise Search Strategies at Networking Events: &nbsp;An Exploratory Study Using Sociometric Badges</a></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Balint Dioszegi</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">15:45</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span>The ConfFlow Application: </span><span class="c39">Encouraging New Diverse Collaborations by Helping Researchers Find Each Other at a Conference</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c6">Hayley Hung</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c10 c8">15:50</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9 c29"><span class="c10 c8">Coffee Break</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c8 c40">Chair: Bernd Dudzik</span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">16:05</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">(Panel) Discussion: &nbsp;The main aim of this discussion is to reflect on the talks of the day and to discuss how to build technologies and carry out research to support spontaneous interactions</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9 c21"><span class="c6"></span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">17.10</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">Drinks reception and poster session</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9 c21"><span class="c6"></span></p></td></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">18:15</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span class="c6">Group walk to the dinner location</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9 c21"><span class="c6"></span></p></td><tbody></tbody></tr><tr class="c12"><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c6">18:30 </span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c9"><span>Dinner in town </span><span class="c16"><a class="c3" href="https://www.google.com/url?q=https://huszar.nl/en/&amp;sa=D&amp;source=editors&amp;ust=1682427983756449&amp;usg=AOvVaw3D8KIgh6wpsuYymBeIHj91">(Huszar</a></span><span class="c6">) located 5 minutes from Delft Central Station and a 15 minute walk from Vakwerkhuis.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9 c21"><span class="c6"></span></p></td></tr></thead></table><h4 class="c0" id="h.svdn3qvno0ni"><span class="c2">PhD Thesis Defense of Stephanie Tan on May 3</span></h4><p class="c14"><span class="c6">The PhD thesis defense of Stephanie Tan will start at 9.30 (layman&rsquo;s presentation) before the defense at 10am in the Aula on the campus of TUDelft.</span></p><h2 class="c38 c29 c15" id="h.2gbt7c8ah9zb"><span class="c13">Invited Talks</span></h2><h4 class="c1 c15" id="h.xwayb5vb9b6j"><span class="c8">Talk Title:</span><span>&nbsp;Studies on Social Interaction using Wearables and Theatre<br></span><span class="c8">Speaker: </span><span>&nbsp;</span><span class="c16"><a class="c3" href="https://www.google.com/url?q=https://jamieward.net/&amp;sa=D&amp;source=editors&amp;ust=1682427983757376&amp;usg=AOvVaw2eaBr9MBP-bKjSCRnpndYD">Dr Jamie A Ward</a></span><span class="c2">, Senior Lecturer in Computer Science at Goldsmiths University of London</span></h4><p class="c1"><span class="c8">Abstract: </span><span class="c6">Measuring detailed information on how people move, see, and think during realistic social situations can be a powerful method in studying social behaviour and cognition. However, &nbsp;measurement-driven research can be limited by the available technology, &nbsp;with bulky equipment and rigid constraints often confining such work to the laboratory, thus limiting the ecological validity of any findings. &nbsp;Together with colleagues at Goldsmiths, UCL, and Keio University, I have been working on several projects that use wearable sensing to take this research out of the laboratory and into the real world -- while on the way, stopping off at the theatre. In this talk, I will give a brief overview of some of our work, and try to show how the paradigm of &nbsp;&#39;theatre as a laboratory&#39;, might provide a way forward, both for &nbsp;research in social cognition, and in wearable sensing.</span></p><h4 class="c1 c15" id="h.yi04rxhqk484"><span class="c8">Talk Title:</span><span>&nbsp;Robots within Groups of People<br></span><span class="c33 c8 c37">Speaker: </span><span class="c33 c37">&nbsp;</span><span class="c16 c33"><a class="c3" href="https://www.google.com/url?q=http://xavirema.eu/&amp;sa=D&amp;source=editors&amp;ust=1682427983757779&amp;usg=AOvVaw2r1dJL6oDuNrp4nR1XQvwl">Xavier Alameda-Pineda</a></span><span class="c2">, INRIA, France</span></h4><p class="c1"><span class="c8">Abstract: </span><span>One of the prominent applications of understanding social human behavior is the development of systems that can interpret, react to, and synthesize behavioral cues, and therefore take and be part of social interactions. In this very general context, social autonomous systems, e.g. social robotics, are a very challenging and complex research area that has received increasing attention over the past years. In this talk, I will be discussing the conception of machine learning methods allowing to perceive, generate and synthesize certain human behavioral cues. The tackled tasks will range from speech enhancement to meta-training for social navigation, and for each of them I will focus on one technical detail that is crucial for the conception of the machine learning model and associated training algorithm.</span></p><h4 class="c1 c15" id="h.gkwxxgl0hets"><span class="c8">Talk Title:</span><span>&nbsp;Understanding Expertise Search Strategies at Networking Events: &nbsp;An Exploratory Study Using Sociometric Badges<br></span><span class="c33 c8 c37">Speaker: </span><span class="c16 c33"><a class="c3" href="https://www.google.com/url?q=https://www.linkedin.com/in/b%25C3%25A1lint-di%25C3%25B3szegi-8292734b/?originalSubdomain%3Duk&amp;sa=D&amp;source=editors&amp;ust=1682427983758205&amp;usg=AOvVaw2LbptS4j65PPhy27jzAmHf">Balint Dioszegi</a></span><span class="c33 c37">, University of Greenwich, UK</span></h4><p class="c1"><span class="c8">Abstract: </span><span class="c6">In this study we ask how individuals search for experts at networking events. Building on the intuition that individuals&rsquo; propensities to engage in certain search actions, as well as their effectiveness in locating experts, will depend on the quality and salience of the metaknowledge they have about others, we conducted an expert search game as a field experiment in which we randomly assigned participants &ndash; researchers in a multinational corporation &ndash; to one of three treatment conditions, reflecting varying degrees of search planning. Based on data from sociometric badges, we derive a taxonomy of the micro-decisions individuals make at events. We find that letting others approach yields more referrals than taking the initiative in starting conversations, and that planning increases the tendency to maintain such initiative even when doing so is ineffective &ndash; a possible manifestation of the Einstellung effect.</span></p><h4 class="c1 c15" id="h.on2dthapwzsb"><span class="c8">Talk Title:</span><span>&nbsp;Towards gaze analysis in the wild<br></span><span class="c33 c8 c37">Speaker: </span><span class="c16 c33"><a class="c3" href="https://www.google.com/url?q=https://www.idiap.ch/~odobez/&amp;sa=D&amp;source=editors&amp;ust=1682427983758647&amp;usg=AOvVaw3i9UXIBhuns6oc01UV9N1y">Jean-Marc Odobez</a></span><span class="c2">, Idiap Research Institute and EPFL, Switzerland</span></h4><p class="c22"><span class="c8">Abstract: </span><span class="c6">As a display of attention and interest, gaze is a fundamental cue in understanding people&#39;s activities, behaviors, and state of mind, and plays an important role in many applications and research fields, for the design of intuitive human computer or robot interfaces, or for medical diagnosis like for assessing Autism Spectrum Disorders (ASD) in children. Gaze (estimating the 3D line of sight) and Visual Focus of Attention (VFOA) estimation, however, are challenging, even for humans. It often requires not only analysing the person&#39;s face and eyes, but also the scene content including the 3D scene structure and the person&rsquo;s situation to detect obstructions in the line of sight or apply attention priors that humans typically have when observing others. In this presentation, I will present methods that address these challenges: first, a method that leverages standard activity-related priors about gaze to perform online calibration; secondly, an approach for VFOA inference which casts the scene in the 3D field of view of a person, enabling the use of audio-visual information as well as dealing with an arbitrary number of targets; and third, moving towards gaze estimation in the wild, an approach for the gaze-following task explicitly leveraging derived multimodal cues like depth and pose.</span></p><p class="c1 c21"><span class="c6"></span></p><h4 class="c1 c15" id="h.7q9szsni17p9"><span class="c8">Talk Title:</span><span>&nbsp;Socially significant bodily rhythms<br></span><span class="c8">Speaker: </span><span class="c16"><a class="c3" href="https://www.google.com/url?q=https://wimpouw.com/&amp;sa=D&amp;source=editors&amp;ust=1682427983759150&amp;usg=AOvVaw1pHmRwwnlyATvjsHDBO3qj">Wim Pouw</a></span><span class="c2">, Radboud University, Netherlands</span></h4><p class="c1"><span class="c8">Abstract: </span><span class="c6">TBA</span></p><p class="c1 c21"><span class="c6"></span></p><h2 class="c1 c15" id="h.35tfy9xgnrgx"><span class="c13">MINGLE Project Talks</span></h2><h4 class="c1 c15" id="h.ni0z9p3anb89"><span class="c8">Talk Title: </span><span class="c27">Head and Body behaviour Estimation with F-formations</span><span><br></span><span class="c8">Speaker: </span><span class="c2">&nbsp;Stephanie Tan</span></h4><p class="c1"><span class="c10 c8">Abstract: </span></p><p class="c1"><span class="c42">In recent years, new domains such as social signal processing and social computing have emerged at the intersection of computer science, human behavioral modeling, and robotics. The aim of these fields is to achieve machine perception of social intelligence, such as understanding behavioral cues of humans (e.g., body language) and complex social relations and attitudes (e.g., dominance, rapport). Challenges towards building such systems include data acquisition with appropriate sensing capabilities for capturing in-the-wild human data, as well as modelling approaches that account for data from multiple modalities (vision, audio, motion) and address context-awareness. In light of these challenges, I will present my work on (1) head and body orientation estimation using sparse weak labels from wearable sensors, (2) joint head orientation estimation in conversation groups, and (3) conversation group detection in social interaction scenes, in addition to an overview on the related data-oriented contributions. I motivate these three tasks from the perspective of individual-level, group-level, and scene-level behavior understanding, and conclude with some open questions related to automated analysis of social behaviors. &nbsp;</span></p><p class="c1"><span class="c8 c10">Associated Paper(s):</span></p><p class="c1"><span class="c23 c4">S. Tan, D. M. J. Tax, H. Hung, Conversation group detection with spatio-temporal context,Proceedings of 2022 International Conference on Multimedia (ICMI) (2022), Pages 170&ndash;180, Oral Presentation</span></p><p class="c1"><span class="c23 c4">S. Tan, D. M. J. Tax, H. Hung, Multimodal joint head orientation estimation in interacting groups via proxemics and interaction dynamics, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) (2021), Vol.5, No.1, 1-22.</span></p><p class="c1"><span class="c23 c4">S. Tan, D. M. J. Tax, H. Hung, Head and body orientation estimation with sparse weak labels in free standing conversational settings, Understanding Social Behavior in Dyadic and Small Group Interactions, Proceedings of Machine Learning Research (2021), 179-203. Presented at ICCV 2021 Understanding Social Behavior in Dyadic and Small Group Interactions Workshop</span></p><p class="c1 c21"><span class="c6"></span></p><p class="c1 c21"><span class="c6"></span></p><p class="c1 c21"><span class="c6"></span></p><h4 class="c1 c15" id="h.e5sevkx583y9"><span class="c8">Talk Title:Data Collection and Annotation of Complex Conversational Scenes</span><span><br></span><span class="c8">Speaker: </span><span class="c2">&nbsp;Jose Vargas Quiros and Chirag Raman</span></h4><p class="c1"><span class="c10 c8">Abstract:</span></p><p class="c1"><span class="c10 c8">Associated Paper(s):</span></p><p class="c1"><span class="c4">Quiros, J. V., Tan, S., Raman, C., Cabrera-Quiros, L., &amp; Hung, H. (2022, March). Covfee: an extensible web framework for continuous-time annotation of human behavior. In </span><span class="c4">Understanding Social Behavior in Dyadic and Small Group Interactions</span><span class="c23 c4">&nbsp;(pp. 265-293). PMLR.</span></p><p class="c1"><span class="c4">Raman, C., Tan, S., &amp; Hung, H. (2020, October). A modular approach for synchronized wireless multimodal multisensor data acquisition in highly dynamic social settings. In </span><span class="c4">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c23 c4">&nbsp;(pp. 3586-3594).</span></p><p class="c1"><span class="c4">Raman, C., Vargas Quiros, J., Tan, S., Islam, A., Gedik, E., &amp; Hung, H. (2022). ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine Analysis of Free-Standing Social Interactions in the Wild. </span><span class="c4">Advances in Neural Information Processing Systems</span><span class="c4">, </span><span class="c4">35</span><span class="c23 c4">, 23701-23715.</span></p><p class="c1"><span class="c4">Raman, C., Tan, S., &amp; Hung, H. (2020, October). A modular approach for synchronized wireless multimodal multisensor data acquisition in highly dynamic social settings. In </span><span class="c4">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c23 c4">&nbsp;(pp. 3586-3594).</span></p><h4 class="c1 c17 c15" id="h.ehxysfraymph"><span class="c2"></span></h4><h4 class="c1 c15" id="h.ibjqkwi20om9"><span class="c8">Talk Title: F-formation Modelling and Behavioural Cue Forecasting.</span><span><br></span><span class="c8">Speaker: </span><span class="c2">&nbsp;Chirag Raman</span></h4><p class="c1"><span class="c10 c8">Abstract:</span></p><p class="c1"><span class="c10 c8">Associated Paper(s):</span></p><p class="c1"><span class="c4">Raman, C., Hung, H., &amp; Loog, M. (2023, February). Social processes: Self-supervised meta-learning over conversational groups for forecasting nonverbal social cues. In Computer Vision&ndash;ECCV 2022 Workshops: Tel Aviv, Israel, October 23&ndash;27, 2022, Proceedings, Part III (pp. 639-659). Cham: Springer Nature Switzerland.</span></p><h4 class="c1 c17 c15" id="h.68svupq4mdnd"><span class="c25 c8"></span></h4><h4 class="c1 c15" id="h.pj43nnn7mbbb"><span class="c8">Talk Title: Estimating Conversational Enjoyment and Learning Multiple Truths about Laughter</span><span><br></span><span class="c8">Speaker: </span><span class="c2">&nbsp;Chirag Raman and Hayley Hung</span></h4><p class="c1"><span class="c10 c8">Abstract:</span></p><p class="c1"><span class="c10 c8">Associated Paper(s):</span></p><p class="c1"><span class="c4">Raman, C., Prabhu, N. R., &amp; Hung, H. (2023). Perceived Conversation Quality in Spontaneous Interactions. </span><span class="c23 c4">IEEE Transactions on Affective Computing</span></p><p class="c1"><span class="c4">Vargas-Quiros, J., Cabrera-Quiros, L., Oertel, C., &amp; Hung, H. (2022). Impact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild. arXiv preprint arXiv:2211.00794. To appear, IEEE Transactions on Affective Computing</span></p><h4 class="c1 c17 c15" id="h.6ltyb6lvuyw2"><span class="c25 c8"></span></h4><h4 class="c1 c17 c15" id="h.jn4ivz8tdx76"><span class="c25 c8"></span></h4><h4 class="c1 c15 c17" id="h.5y5yk8iwr3u9"><span class="c8 c25"></span></h4><h4 class="c1 c15" id="h.xq5n2el5kwma"><span class="c8">Talk Title: The ConfFlow Application: Encouraging New Diverse Collaborations by Helping Researchers Find Each Other at a Conference</span><span><br></span><span class="c8">Speaker: </span><span class="c2">&nbsp;Hayley Hung</span></h4><p class="c1"><span class="c8">Abstract: </span><span class="c20">We often find other collaborators by chance at a conference or by looking for them specifically through their papers. However, sometimes hidden potential social connections might exist between different researchers that cannot be immediately observed because the keywords we use might not always represent the entire space of similar research interests.ConfFlow is an online application that offers an alternative perspective on finding new research connections. It is designed to help researchers find others at conferences with complementary research interests for collaboration. With ConfFlow we take a data-driven approach by using something similar to the Toronto Paper Matching System (TPMS), used to identify suitable reviewers for papers, to construct a similarity embedding space for researchers to find other researchers. </span></p><p class="c1"><span class="c8 c43">Associated Paper(s):</span><span><br></span><span class="c4">H. Hung and E. Gedik, &ldquo;Encouraging Scientific Collaborations with ConfFlow 2021&rdquo;, SIGMM Records, </span><span class="c4 c16"><a class="c3" href="https://www.google.com/url?q=https://records.sigmm.org/2022/04/20/encouraging-scientific-collaborations-with-confflow-2021/&amp;sa=D&amp;source=editors&amp;ust=1682427983762574&amp;usg=AOvVaw2fkjdwLLRK7O2i388U6ZH4">https://records.sigmm.org/2022/04/20/encouraging-scientific-collaborations-with-confflow-2021/</a></span><span class="c23 c4">, 2022</span></p><p class="c1"><span class="c4">H.Hung and E.Gedik, &ldquo;Encouraging more Diverse Scientific Collaborations with the ConfFlow application&rdquo;, SIGMM Records, </span><span class="c4"><a class="c3" href="https://www.google.com/url?q=https://records.sigmm.org/2021/06/10/encouraging-more-diverse-scientific-collaborations-with-the-confflow-application/&amp;sa=D&amp;source=editors&amp;ust=1682427983762891&amp;usg=AOvVaw3MkmW0AsZndBhfyQNnWzEX">https://records.sigmm.org/2021/06/10/encouraging-more-diverse-scientific-collaborations-with-the-confflow-application/</a></span><span class="c23 c4">, 2021</span></p><p class="c14"><span class="c4">Gedik, E., &amp; Hung, H. (2020, October). ConfFlow: A Tool to Encourage New Diverse Collaborations. In </span><span class="c4">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c4">&nbsp;(pp. 4562-4564).</span></p><p class="c1 c21"><span class="c23 c4"></span></p><h2 class="c1 c15 c18" id="h.t8ene8slce3y"><span class="c23 c4"></span></h2><p class="c14 c21"><span class="c6"></span></p><h2 class="c1 c15 c18" id="h.5htxyzvbch1o"><span class="c13"></span></h2><h2 class="c1 c15" id="h.r0r4rnwmcv0z"><span><br></span><span class="c13">Poster Presentations</span></h2><h4 class="c1 c15" id="h.rb9ojqsjmy31"><span class="c2">Presenter name and affiliation:<br>Poster Title and Abstract:TBA</span></h4><h4 class="c1 c17 c15" id="h.2ou0r5v8ze2o"><span class="c2"></span></h4></body></html>
<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA');.lst-kix_62gtr7dgmd8s-6>li:before{content:"\0025cf  "}.lst-kix_62gtr7dgmd8s-7>li:before{content:"\0025cb  "}.lst-kix_62gtr7dgmd8s-0>li:before{content:"\0025cf  "}.lst-kix_62gtr7dgmd8s-8>li:before{content:"\0025a0  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_62gtr7dgmd8s-4{list-style-type:none}.lst-kix_62gtr7dgmd8s-2>li:before{content:"\0025a0  "}.lst-kix_62gtr7dgmd8s-3>li:before{content:"\0025cf  "}ul.lst-kix_62gtr7dgmd8s-5{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-2{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-3{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-0{list-style-type:none}.lst-kix_62gtr7dgmd8s-1>li:before{content:"\0025cb  "}ul.lst-kix_62gtr7dgmd8s-1{list-style-type:none}.lst-kix_62gtr7dgmd8s-5>li:before{content:"\0025a0  "}ul.lst-kix_62gtr7dgmd8s-8{list-style-type:none}.lst-kix_62gtr7dgmd8s-4>li:before{content:"\0025cb  "}ul.lst-kix_62gtr7dgmd8s-6{list-style-type:none}ul.lst-kix_62gtr7dgmd8s-7{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:134.2pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.2pt;border-top-color:#000000;border-bottom-style:solid}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:285.8pt;border-top-color:#000000;border-bottom-style:solid}.c9{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c2{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c6{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c10{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c14{padding-top:12pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;text-align:left;height:12pt}.c41{color:#3c4043;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Roboto";font-style:normal}.c23{padding-top:12pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;text-align:left;height:16pt}.c22{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c21{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c4{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c19{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c27{padding-top:12pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;text-align:left}.c28{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c3{background-color:#ffffff;font-size:8pt;font-family:"Times New Roman";font-weight:400}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c46{border-spacing:0;border-collapse:collapse;margin-right:auto}.c25{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c34{text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c30{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c39{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c26{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c0{font-size:12pt;color:#666666;font-weight:700}.c37{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c44{background-color:#ffffff;font-size:9pt;color:#333333}.c31{font-weight:400;font-size:10pt;font-family:"Arial"}.c32{font-weight:400;font-size:20pt;font-family:"Arial"}.c42{font-family:"Roboto";color:#3c4043;font-weight:400}.c47{color:#000000;font-size:11pt}.c29{font-size:9pt;font-style:italic}.c45{font-weight:400;font-family:"Roboto"}.c35{background-color:#ffffff;color:#222222}.c48{vertical-align:baseline;font-style:normal}.c8{orphans:2;widows:2}.c40{color:#666666;font-size:12pt}.c38{padding:0;margin:0}.c36{background-color:#ffffff;font-size:10.5pt}.c13{color:inherit;text-decoration:inherit}.c15{height:11pt}.c43{font-size:12pt}.c17{height:0pt}.c49{font-size:11pt}.c18{font-weight:700}.c33{height:16pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c37 doc-content"><h1 class="c2" id="h.2yawkmmk6pbg"><span class="c25 c32">Interdisciplinary Perspectives on Technologies for Mingling</span></h1><h4 class="c21 c8" id="h.tmptoux9f1tn"><span class="c10">The Future of Conversations and Mingling </span></h4><p class="c30 c8"><span class="c11">Time: 2 May 2023</span></p><p class="c30 c8"><span>Location : &nbsp;</span><span class="c26"><a class="c13" href="https://www.google.com/url?q=https://vakwerkhuis.com/&amp;sa=D&amp;source=editors&amp;ust=1682333562120590&amp;usg=AOvVaw03LHZPV0xyYHXYqKM2qQYJ">Vakwerkhuis, Delft </a></span></p><p class="c7"><span class="c11"></span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 538.00px; height: 303.00px;"><img alt="" src="images/image1.png" style="width: 538.00px; height: 303.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c19 c8 c33" id="h.bf7z376z34o6"><span class="c24"></span></h2><p class="c4"><span class="c9"><a class="c13" href="#h.uobv7junyb4f">Registration and Participation:</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.gg8m22e8uqaw">Introduction</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.l3c9d5n4e4o4">Motivation</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.og4jawi1xhf5">Bring your posters!</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.5rwur0w63m4d">Schedule</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.2gbt7c8ah9zb">Invited Talks</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.35tfy9xgnrgx">MINGLE Project Talks</a></span></p><p class="c4"><span class="c9"><a class="c13" href="#h.r0r4rnwmcv0z">Poster Presentations</a></span></p><h2 class="c19 c8 c33" id="h.4pwtykwvsx7b"><span class="c24"></span></h2><h2 class="c19 c8" id="h.uobv7junyb4f"><span class="c24">Registration and Participation: </span></h2><p class="c30 c8"><span>Please register </span><span class="c26"><a class="c13" href="https://www.google.com/url?q=https://docs.google.com/forms/d/e/1FAIpQLSdii11cRmcQ9Y-fJQTgnAI8FK64ngxShm0_I27oHdR8tEsXLQ/viewform&amp;sa=D&amp;source=editors&amp;ust=1682333562123013&amp;usg=AOvVaw0wxle_jNQ8wGkuBReYqRhk">here </a></span><span class="c18">before April 25</span><span class="c11">. The main idea is to use this as a networking and idea exchange opportunity so registration is free!</span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span class="c11">The main aim of this event is to bring together researchers and stakeholders who may not have become united by such a common theme before. So physical presence is strongly encouraged. However, if you would like to join remotely, here is the link:</span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span class="c36 c41">Join Zoom Meeting</span></p><p class="c30 c8"><span class="c26 c36 c45"><a class="c13" href="https://www.google.com/url?q=https://tudelft.zoom.us/j/97141920397?pwd%3DamxlMmR1d244b0piT0c0MnpXRXAxZz09&amp;sa=D&amp;source=editors&amp;ust=1682333562123686&amp;usg=AOvVaw0oAth4WrnTvdd8GiavGuFi">https://tudelft.zoom.us/j/97141920397?pwd=amxlMmR1d244b0piT0c0MnpXRXAxZz09</a></span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span class="c41 c36">Meeting ID: 971 4192 0397</span></p><p class="c8 c30"><span class="c36 c42">Passcode: 661783</span></p><h2 class="c19 c8" id="h.gg8m22e8uqaw"><span class="c24">Introduction</span></h2><p class="c30 c8"><span class="c11">Attending social networking events has been correlated with career success. Yet little is known about how or why they function well or how we could make them more useful for us. This is despite the fact that we spend substantial time and money to attend them. One of the major bottlenecks has been related to difficulties in observing such behaviour systematically. This has made it hard to develop theories to fully understand what happens in these crowds. Without the possibility to analyse them, technologies cannot be built to help us to make the most out of these experiences which can sometimes be anxiety inducing for some. </span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span class="c11">During the global pandemic of recent years, these spontaneous moments of conversational interaction were lost and led to people questioning whether we should bring serendipity and spontaneity back in other forms. What makes a conversation good? What makes them interesting enough to form new bonds or foster existing connections? How does this play out in groups? </span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span class="c11">This symposium aims to crack open the mysteries of mingling behaviour from multiple different perspectives. Moreover, understanding how to build technologies for such settings would enable us to bridge the gap in understanding behaviours in similar and even more commonplace activities such as the role of spontaneous discussions by the coffee machine at work or in public spaces. </span></p><h2 class="c19 c8" id="h.l3c9d5n4e4o4"><span class="c24">Motivation</span></h2><p class="c30 c8"><span class="c11">The day aims to present an overarching view of the research results of the NWO funded Vidi project MINGLE (Modelling Social Group Dynamics and Interaction Quality in Complex Scenes using Multi-Sensor Analysis of Non-Verbal Behaviour). Whilst this is the closing event of the MINGLE project, it is also aimed as a new beginning. The research results of MINGLE will feed into a new ERC Consolidator grant funded project NEON (Nonverbal Intention Modelling) which will focus on the analysis of intention, particularly &nbsp;in mingling settings. So we are looking for new perspectives to enrich the new research journey.</span></p><p class="c7"><span class="c11"></span></p><p class="c30 c8"><span class="c11">Since this is a first of its kind event, we want to kickstart a new kind of community that explores important research questions, solutions, and needs that can aid spontaneous social connection making. </span></p><h2 class="c19 c8" id="h.og4jawi1xhf5"><span class="c24">Bring your posters!</span></h2><p class="c30 c8"><span>Do you work on a </span><span>related</span><span class="c11">&nbsp;topic? topics include but are not limited to:</span></p><ul class="c38 lst-kix_62gtr7dgmd8s-0 start"><li class="c22 c8 li-bullet-0"><span>Individual and Social </span><span>Behaviour</span><span class="c11">&nbsp;Understanding (e.g. gaze, gestures, postures, speech detection, paralinguistics, multimodal behaviour analysis)</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Cognitive states (e.g. affect, memory, trust)</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Cooperation and Collaboration in groups, organizations</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Interpersonal factors: engagement, rapport, synchrony, and mimicry</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Ethics and Privacy issues</span></li><li class="c8 c22 li-bullet-0"><span class="c11">Interactive agents (e.g. social robots, virtual agents)</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Wearable/ubiquitous sensing</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Data collection techniques</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Data annotation techniques</span></li><li class="c22 c8 li-bullet-0"><span class="c11">Technologies to enable or enhance social interactions</span></li></ul><h2 class="c19 c8" id="h.5rwur0w63m4d"><span class="c24">Schedule </span></h2><p class="c30 c8"><span class="c11">Please note the schedule is not yet finalised and may be subject to slight shifts!</span></p><a id="t.ee56856513e9432565079eac73e763e8f7c61da8"></a><a id="t.0"></a><table class="c46"><thead><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">Time</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c15"><span class="c11"></span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Speaker</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c1">9:15</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Walk in: tea, coffee, and refreshments are available</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c29 c18 c39">Chair: Chirag Raman</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">9:45</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">Opening</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Hayley Hung </span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">10:00</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c30 c8"><span class="c26"><a class="c13" href="#h.ni0z9p3anb89">Head and Body behaviour Estimation with F-formations</a></span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Stephanie Tan</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">10:20</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span class="c26"><a class="c13" href="#h.xwayb5vb9b6j">Studies on Social Interaction using Wearables and Theatre</a></span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Jamie A Ward</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c1">10:55</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Coffee Break/ hang poster</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c39 c29 c18">Chair: Hayley Hung</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">11:10</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">Data Collection and Annotation of Complex Conversational Scenes</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Jose Vargas Quiros and &nbsp;Chirag Raman</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">11:30</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span class="c11">Talk Title: TBA</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Jean-Marc Odobez</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c1">12:05</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Buffet Lunch</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c39 c29 c18">Chair: Hayley Hung</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">13:05</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span class="c26"><a class="c13" href="#h.yi04rxhqk484">Robots within Groups of People</a></span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Xavier Alameda Pineda</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">13:40</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">F-formation Modelling and Behavioural Cue Forecasting.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Chirag Raman</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">14:00</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span class="c26"><a class="c13" href="#h.7q9szsni17p9">Socially significant bodily rhythms</a></span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Wim Pouw</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">14:35</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">Coffee Break</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c39 c29 c18">Chair: Bernd Dudzik</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">14:50</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">Estimating Conversational Enjoyment and Learning Multiple Truths about Laughter</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Chirag Raman and Hayley Hung</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">15:10</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span class="c26"><a class="c13" href="#h.gkwxxgl0hets">Understanding Expertise Search Strategies at Networking Events: &nbsp;An Exploratory Study Using Sociometric Badges</a></span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Balint Dioszegi</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">15:45</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span>The ConfFlow Application: </span><span class="c36">Encouraging New Diverse Collaborations by Helping Researchers Find Each Other at a Conference</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c11">Hayley Hung</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c1">15:50</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5 c8"><span class="c1">Coffee Break</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5"><span class="c18 c29">Chair: Bernd Dudzik</span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">16:05</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">(Panel) Discussion: &nbsp;The main aim of this discussion is to reflect on the talks of the day and to discuss how to build technologies and carry out research to support spontaneous interactions</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5 c15"><span class="c11"></span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">17.10</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">Drinks reception and poster session</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5 c15"><span class="c11"></span></p></td></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">18:15</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c11">Group walk to the dinner location</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5 c15"><span class="c11"></span></p></td><tbody></tbody></tr><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c11">18:30 </span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span>Dinner in town </span><span class="c26"><a class="c13" href="https://www.google.com/url?q=https://huszar.nl/en/&amp;sa=D&amp;source=editors&amp;ust=1682333562148731&amp;usg=AOvVaw2uKM3vg3KIAS571TsWLp29">(Huszar</a></span><span class="c11">) located 5 minutes from Delft Central Station and a 15 minute walk from Vakwerkhuis.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c5 c15"><span class="c11"></span></p></td></tr></thead></table><h4 class="c8 c21" id="h.svdn3qvno0ni"><span class="c10">PhD Thesis Defense of Stephanie Tan on May 3</span></h4><p class="c30 c8"><span class="c11">The PhD thesis defense of Stephanie Tan will start at 9.30 (layman&rsquo;s presentation) before the defense at 10am in the Aula on the campus of TUDelft.</span></p><h2 class="c8 c19" id="h.2gbt7c8ah9zb"><span class="c24">Invited Talks</span></h2><h4 class="c27 c8" id="h.xwayb5vb9b6j"><span class="c18">Talk Title:</span><span>&nbsp;Studies on Social Interaction using Wearables and Theatre<br></span><span class="c18">Speaker: </span><span class="c10">&nbsp;Dr Jamie A Ward, Senior Lecturer in Computer Science at Goldsmiths University of London</span></h4><p class="c28 c8"><span class="c18">Abstract: </span><span class="c11">Measuring detailed information on how people move, see, and think during realistic social situations can be a powerful method in studying social behaviour and cognition. However, &nbsp;measurement-driven research can be limited by the available technology, &nbsp;with bulky equipment and rigid constraints often confining such work to the laboratory, thus limiting the ecological validity of any findings. &nbsp;Together with colleagues at Goldsmiths, UCL, and Keio University, I have been working on several projects that use wearable sensing to take this research out of the laboratory and into the real world -- while on the way, stopping off at the theatre. In this talk, I will give a brief overview of some of our work, and try to show how the paradigm of &nbsp;&#39;theatre as a laboratory&#39;, might provide a way forward, both for &nbsp;research in social cognition, and in wearable sensing.</span></p><h4 class="c27 c8" id="h.yi04rxhqk484"><span class="c18">Talk Title:</span><span>&nbsp;Robots within Groups of People<br></span><span class="c0">Speaker: </span><span class="c40">&nbsp;</span><span class="c26 c43"><a class="c13" href="https://www.google.com/url?q=http://xavirema.eu/&amp;sa=D&amp;source=editors&amp;ust=1682333562150679&amp;usg=AOvVaw3EX6vFlL2zjEvC7NZoWh6l">Xavier Alameda-Pineda</a></span><span class="c10">, INRIA, France</span></h4><p class="c28 c8"><span class="c18">Abstract: </span><span>One of the prominent applications of understanding social human behavior is the development of systems that can interpret, react to, and synthesize behavioral cues, and therefore take and be part of social interactions. In this very general context, social autonomous systems, e.g. social robotics, are a very challenging and complex research area that has received increasing attention over the past years. In this talk, I will be discussing the conception of machine learning methods allowing to perceive, generate and synthesize certain human behavioral cues. The tackled tasks will range from speech enhancement to meta-training for social navigation, and for each of them I will focus on one technical detail that is crucial for the conception of the machine learning model and associated training algorithm.</span></p><h4 class="c27 c8" id="h.gkwxxgl0hets"><span class="c18">Talk Title:</span><span>&nbsp;Understanding Expertise Search Strategies at Networking Events: &nbsp;An Exploratory Study Using Sociometric Badges<br></span><span class="c0">Speaker: </span><span class="c26 c43"><a class="c13" href="https://www.google.com/url?q=https://www.linkedin.com/in/b%25C3%25A1lint-di%25C3%25B3szegi-8292734b/?originalSubdomain%3Duk&amp;sa=D&amp;source=editors&amp;ust=1682333562151327&amp;usg=AOvVaw0TAPYNHmt-xmHvPYY3pG6o">Balint Dioszegi</a></span><span class="c40">, University of Greenwich, UK</span></h4><p class="c28 c8"><span class="c18">Abstract: </span><span class="c11">In this study we ask how individuals search for experts at networking events. Building on the intuition that individuals&rsquo; propensities to engage in certain search actions, as well as their effectiveness in locating experts, will depend on the quality and salience of the metaknowledge they have about others, we conducted an expert search game as a field experiment in which we randomly assigned participants &ndash; researchers in a multinational corporation &ndash; to one of three treatment conditions, reflecting varying degrees of search planning. Based on data from sociometric badges, we derive a taxonomy of the micro-decisions individuals make at events. We find that letting others approach yields more referrals than taking the initiative in starting conversations, and that planning increases the tendency to maintain such initiative even when doing so is ineffective &ndash; a possible manifestation of the Einstellung effect.</span></p><h4 class="c27 c8" id="h.on2dthapwzsb"><span class="c18">Talk Title:</span><span>&nbsp;TBA<br></span><span class="c0">Speaker: </span><span class="c26 c43"><a class="c13" href="https://www.google.com/url?q=https://www.idiap.ch/~odobez/&amp;sa=D&amp;source=editors&amp;ust=1682333562152044&amp;usg=AOvVaw1vbhSCjnvDuyF0UUZ1285I">Jean-Marc Odobez</a></span><span class="c10">, Idiap Research Institute and EPFL, Switzerland</span></h4><p class="c28 c8"><span class="c18">Abstract: </span><span class="c11">TBA</span></p><p class="c6"><span class="c11"></span></p><h4 class="c27 c8" id="h.7q9szsni17p9"><span class="c18">Talk Title:</span><span>&nbsp;Socially significant bodily rhythms<br></span><span class="c18">Speaker: </span><span class="c26"><a class="c13" href="https://www.google.com/url?q=https://wimpouw.com/&amp;sa=D&amp;source=editors&amp;ust=1682333562152703&amp;usg=AOvVaw39UBlQjRMvlKA0zQi072xB">Wim Pouw</a></span><span class="c10">, Radboud University, Netherlands</span></h4><p class="c28 c8"><span class="c18">Abstract: </span><span class="c11">TBA</span></p><p class="c6"><span class="c11"></span></p><h2 class="c8 c27" id="h.35tfy9xgnrgx"><span class="c24">MINGLE Project Talks</span></h2><h4 class="c27 c8" id="h.ni0z9p3anb89"><span class="c18">Talk Title: </span><span class="c47">Head and Body behaviour Estimation with F-formations</span><span><br></span><span class="c18">Speaker: </span><span class="c10">&nbsp;Stephanie Tan</span></h4><p class="c28 c8"><span class="c1">Abstract: </span></p><p class="c28 c8"><span class="c35">In recent years, new domains such as social signal processing and social computing have emerged at the intersection of computer science, human behavioral modeling, and robotics. The aim of these fields is to achieve machine perception of social intelligence, such as understanding behavioral cues of humans (e.g., body language) and complex social relations and attitudes (e.g., dominance, rapport). Challenges towards building such systems include data acquisition with appropriate sensing capabilities for capturing in-the-wild human data, as well as modelling approaches that account for data from multiple modalities (vision, audio, motion) and address context-awareness. In light of these challenges, I will present my work on (1) head and body orientation estimation using sparse weak labels from wearable sensors, (2) joint head orientation estimation in conversation groups, and (3) conversation group detection in social interaction scenes, in addition to an overview on the related data-oriented contributions. I motivate these three tasks from the perspective of individual-level, group-level, and scene-level behavior understanding, and conclude with some open questions related to automated analysis of social behaviors. &nbsp;</span></p><p class="c28 c8"><span class="c1">Associated Paper(s):</span></p><p class="c28 c8"><span class="c3 c25">S. Tan, D. M. J. Tax, H. Hung, Conversation group detection with spatio-temporal context,Proceedings of 2022 International Conference on Multimedia (ICMI) (2022), Pages 170&ndash;180, Oral Presentation</span></p><p class="c28 c8"><span class="c3 c25">S. Tan, D. M. J. Tax, H. Hung, Multimodal joint head orientation estimation in interacting groups via proxemics and interaction dynamics, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) (2021), Vol.5, No.1, 1-22.</span></p><p class="c28 c8"><span class="c3 c25">S. Tan, D. M. J. Tax, H. Hung, Head and body orientation estimation with sparse weak labels in free standing conversational settings, Understanding Social Behavior in Dyadic and Small Group Interactions, Proceedings of Machine Learning Research (2021), 179-203. Presented at ICCV 2021 Understanding Social Behavior in Dyadic and Small Group Interactions Workshop</span></p><p class="c6"><span class="c11"></span></p><p class="c6"><span class="c11"></span></p><p class="c6"><span class="c11"></span></p><h4 class="c27 c8" id="h.e5sevkx583y9"><span class="c18">Talk Title:Data Collection and Annotation of Complex Conversational Scenes</span><span><br></span><span class="c18">Speaker: </span><span class="c10">&nbsp;Jose Vargas Quiros and Chirag Raman</span></h4><p class="c28 c8"><span class="c1">Abstract:</span></p><p class="c28 c8"><span class="c1">Associated Paper(s):</span></p><p class="c28 c8"><span class="c3">Quiros, J. V., Tan, S., Raman, C., Cabrera-Quiros, L., &amp; Hung, H. (2022, March). Covfee: an extensible web framework for continuous-time annotation of human behavior. In </span><span class="c3">Understanding Social Behavior in Dyadic and Small Group Interactions</span><span class="c3 c25">&nbsp;(pp. 265-293). PMLR.</span></p><p class="c28 c8"><span class="c3">Raman, C., Tan, S., &amp; Hung, H. (2020, October). A modular approach for synchronized wireless multimodal multisensor data acquisition in highly dynamic social settings. In </span><span class="c3">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c3 c25">&nbsp;(pp. 3586-3594).</span></p><p class="c28 c8"><span class="c3">Raman, C., Vargas Quiros, J., Tan, S., Islam, A., Gedik, E., &amp; Hung, H. (2022). ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine Analysis of Free-Standing Social Interactions in the Wild. </span><span class="c3">Advances in Neural Information Processing Systems</span><span class="c3">, </span><span class="c3">35</span><span class="c3 c25">, 23701-23715.</span></p><p class="c28 c8"><span class="c3">Raman, C., Tan, S., &amp; Hung, H. (2020, October). A modular approach for synchronized wireless multimodal multisensor data acquisition in highly dynamic social settings. In </span><span class="c3">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c3 c25">&nbsp;(pp. 3586-3594).</span></p><h4 class="c14 c8" id="h.ehxysfraymph"><span class="c10"></span></h4><h4 class="c27 c8" id="h.ibjqkwi20om9"><span class="c18">Talk Title: F-formation Modelling and Behavioural Cue Forecasting.</span><span><br></span><span class="c18">Speaker: </span><span class="c10">&nbsp;Chirag Raman</span></h4><p class="c28 c8"><span class="c1">Abstract:</span></p><p class="c8 c28"><span class="c1">Associated Paper(s):</span></p><p class="c28 c8"><span class="c3">Raman, C., Hung, H., &amp; Loog, M. (2023, February). Social processes: Self-supervised meta-learning over conversational groups for forecasting nonverbal social cues. In Computer Vision&ndash;ECCV 2022 Workshops: Tel Aviv, Israel, October 23&ndash;27, 2022, Proceedings, Part III (pp. 639-659). Cham: Springer Nature Switzerland.</span></p><h4 class="c14 c8" id="h.68svupq4mdnd"><span class="c0 c34"></span></h4><h4 class="c27 c8" id="h.pj43nnn7mbbb"><span class="c18">Talk Title: Estimating Conversational Enjoyment and Learning Multiple Truths about Laughter</span><span><br></span><span class="c18">Speaker: </span><span class="c10">&nbsp;Chirag Raman and Hayley Hung</span></h4><p class="c28 c8"><span class="c1">Abstract:</span></p><p class="c28 c8"><span class="c1">Associated Paper(s):</span></p><p class="c28 c8"><span class="c3">Raman, C., Prabhu, N. R., &amp; Hung, H. (2023). Perceived Conversation Quality in Spontaneous Interactions. </span><span class="c3 c25">IEEE Transactions on Affective Computing</span></p><p class="c28 c8"><span class="c3">Vargas-Quiros, J., Cabrera-Quiros, L., Oertel, C., &amp; Hung, H. (2022). Impact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild. arXiv preprint arXiv:2211.00794. To appear, IEEE Transactions on Affective Computing</span></p><h4 class="c8 c14" id="h.6ltyb6lvuyw2"><span class="c34 c0"></span></h4><h4 class="c14 c8" id="h.jn4ivz8tdx76"><span class="c34 c0"></span></h4><h4 class="c14 c8" id="h.5y5yk8iwr3u9"><span class="c34 c0"></span></h4><h4 class="c27 c8" id="h.xq5n2el5kwma"><span class="c18">Talk Title: The ConfFlow Application: Encouraging New Diverse Collaborations by Helping Researchers Find Each Other at a Conference</span><span><br></span><span class="c18">Speaker: </span><span class="c10">&nbsp;Hayley Hung</span></h4><p class="c28 c8"><span class="c18">Abstract: </span><span class="c44">We often find other collaborators by chance at a conference or by looking for them specifically through their papers. However, sometimes hidden potential social connections might exist between different researchers that cannot be immediately observed because the keywords we use might not always represent the entire space of similar research interests.ConfFlow is an online application that offers an alternative perspective on finding new research connections. It is designed to help researchers find others at conferences with complementary research interests for collaboration. With ConfFlow we take a data-driven approach by using something similar to the Toronto Paper Matching System (TPMS), used to identify suitable reviewers for papers, to construct a similarity embedding space for researchers to find other researchers. </span></p><p class="c28 c8"><span class="c18 c49">Associated Paper(s):</span><span><br></span><span class="c3">H. Hung and E. Gedik, &ldquo;Encouraging Scientific Collaborations with ConfFlow 2021&rdquo;, SIGMM Records, </span><span class="c3 c26"><a class="c13" href="https://www.google.com/url?q=https://records.sigmm.org/2022/04/20/encouraging-scientific-collaborations-with-confflow-2021/&amp;sa=D&amp;source=editors&amp;ust=1682333562158796&amp;usg=AOvVaw1irDPeth4wyDG4nimO8t-I">https://records.sigmm.org/2022/04/20/encouraging-scientific-collaborations-with-confflow-2021/</a></span><span class="c3 c25">, 2022</span></p><p class="c28 c8"><span class="c3">H.Hung and E.Gedik, &ldquo;Encouraging more Diverse Scientific Collaborations with the ConfFlow application&rdquo;, SIGMM Records, </span><span class="c3"><a class="c13" href="https://www.google.com/url?q=https://records.sigmm.org/2021/06/10/encouraging-more-diverse-scientific-collaborations-with-the-confflow-application/&amp;sa=D&amp;source=editors&amp;ust=1682333562159169&amp;usg=AOvVaw1lHoho_j05IXfrzn_Vsuds">https://records.sigmm.org/2021/06/10/encouraging-more-diverse-scientific-collaborations-with-the-confflow-application/</a></span><span class="c3 c25">, 2021</span></p><p class="c30 c8"><span class="c3">Gedik, E., &amp; Hung, H. (2020, October). ConfFlow: A Tool to Encourage New Diverse Collaborations. In </span><span class="c3">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="c3">&nbsp;(pp. 4562-4564).</span></p><p class="c6"><span class="c3 c25"></span></p><h2 class="c8 c23" id="h.t8ene8slce3y"><span class="c3 c25"></span></h2><p class="c7"><span class="c11"></span></p><h2 class="c23 c8" id="h.5htxyzvbch1o"><span class="c24"></span></h2><h2 class="c27 c8" id="h.r0r4rnwmcv0z"><span><br></span><span class="c24">Poster Presentations</span></h2><h4 class="c27 c8" id="h.rb9ojqsjmy31"><span class="c10">Presenter name and affiliation:<br>Poster Title and Abstract:TBA</span></h4><h4 class="c14 c8" id="h.2ou0r5v8ze2o"><span class="c10"></span></h4></body></html>